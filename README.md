## ğŸš€ Project Overview

NeuroQuant Agent is a fully custom offline reinforcement learning benchmark, built from the ground up with real-time constraints, compression-aware inference, and deployment to latency-constrained environments.

The project begins with a custom-built 10Ã—10 gridworld environment that supports:

- ğŸ” **Directional movement**: The agent can turn left, go forward, or turn right relative to its current orientation.
- ğŸ‘ï¸ **Partial observability**: Instead of seeing the entire map, the agent receives a 3Ã—3 view centered around its position.
- â›” **Obstacles**: Impassable wall tiles block the agent's path and require navigation.
- ğŸ¯ **Goal tile**: A single terminal state gives a large positive reward when reached, ending the episode.
- ğŸ–¥ï¸ **Real-time PyGame rendering**: Each simulation step is rendered at a capped 10 FPS for visual inspection and timing fidelity.

This environment is used as the basis for:
- Generating offline replay buffers
- Training offline RL agents using CQL, BCQ, or TD3+BC
- Benchmarking model compression tradeoffs (quantization, pruning, distillation)
- Real-time deployment of agents under latency and memory constraints


## ğŸ§  Environment Design

The environment is a 10Ã—10 gridworld with directional agent movement, obstacles, and a single terminal goal. Key features:

- ğŸ” **Action space**: Turn left, move forward, turn right (relative to current orientation)
- ğŸ‘ï¸ **Partial observability**: Agent receives a 3Ã—3 window centered on its current location
- ğŸ”¢ **Dual observation modes**:
  - **Image**: 3Ã—3 local grid (int matrix)
  - **Vector**: Agent position and goal coordinates as a flat vector
- ğŸ¯ **Reward structure**:
  - `+10` for reaching the goal (sparse)
  - `-0.1` per step (dense penalty)
- â›” **Obstacles**: Defined in the grid and block movement
- ğŸ–¥ï¸ **Real-time rendering**: PyGame visualization at 10 FPS

## ğŸ§  Replay Buffer Generation

We simulate random or scripted agents in the custom Gridworld environment to collect experience data for offline RL training.

Each transition includes:
- `observation`
- `action`
- `reward`
- `next_observation`
- `done`

These transitions are saved into a compressed `.npz` buffer (`dataset/replay_buffer.npz`), which can later be loaded for training Conservative Q-Learning (CQL), TD3+BC, or BCQ agents.

Additional outputs include:
- âœ… Episode metadata (average reward, length, and total transitions) saved to `dataset/metadata.txt`
- ğŸ“Š A histogram of reward distribution over episodes saved to `dataset/reward_histogram.png`

To generate the dataset, run:

```bash
python dataset/collect.py --episodes 100
```

This will generate 10k+ transitions across 100 episodes using a random policy.

---

## ğŸ“ Project Structure

```bash
offline-rl-agent/
â”‚
â”œâ”€â”€ env/                    # Custom Gym environment (NeuroQuantEnv)
â”‚   â””â”€â”€ neuroquant_env.py
â”‚
â”œâ”€â”€ dataset/                # Replay buffer collection + visualizations
â”‚   â”œâ”€â”€ collect.py          # Random/scripted policy buffer generation
â”‚   â”œâ”€â”€ viz.py              # t-SNE, reward, and action plots
â”‚   â”œâ”€â”€ replay_buffer.npz   # (gitignored) Collected transitions
â”‚   â”œâ”€â”€ reward_histogram.png
â”‚   â”œâ”€â”€ metadata.txt
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ plots/              # Visual outputs of dataset
â”‚       â”œâ”€â”€ tsne_obs.png
â”‚       â”œâ”€â”€ action_distribution.png
â”‚       â””â”€â”€ episode_rewards.png
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ run_env_test.py         # Debug script to manually interact with env
```

---

## ğŸ“Š Dataset Visualizations

We visualize the replay buffer to verify coverage and distribution:

- ğŸŒ€ [t-SNE of Observations](docs/plots/tsne_obs.png): clusters state embeddings in 2D
- ğŸ® [Action Distribution](docs/plots/action_distribution.png): histogram over agent actions
- ğŸ¯ [Episode Reward Distribution](docs/plots/episode_rewards.png): how returns are spread across episodes

These plots are generated via:

```bash
python dataset/viz.py
```

---

## ğŸ“¦ Getting Started

```bash
# 1. Clone and enter the repo
git clone https://github.com/mansoor-mamnoon/offline-rl-agent.git
cd offline-rl-agent

# 2. Set up virtual environment
python3 -m venv .venv
source .venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Run environment manually
python env/run_env_test.py

# 5. Collect dataset
python dataset/collect.py --episodes 100

# 6. Visualize dataset
python dataset/viz.py
```

---





## ğŸ¥ Demos + GIFs

The environment supports saving full episodes as GIFs using the `render_episode_gif()` function.

Sample run saved to `docs/replays/test_run.gif`:
![Sample Replay](docs/replays/test_run.gif)


