## ğŸš€ Project Overview

NeuroQuant Agent is a fully custom offline reinforcement learning benchmark, built from the ground up with real-time constraints, compression-aware inference, and deployment to latency-constrained environments.

The project begins with a custom-built 10Ã—10 gridworld environment that supports:

- ğŸ” **Directional movement**: The agent can turn left, go forward, or turn right relative to its current orientation.
- ğŸ‘ï¸ **Partial observability**: Instead of seeing the entire map, the agent receives a 3Ã—3 view centered around its position.
- â›” **Obstacles**: Impassable wall tiles block the agent's path and require navigation.
- ğŸ¯ **Goal tile**: A single terminal state gives a large positive reward when reached, ending the episode.
- ğŸ–¥ï¸ **Real-time PyGame rendering**: Each simulation step is rendered at a capped 10 FPS for visual inspection and timing fidelity.

This environment is used as the basis for:
- Generating offline replay buffers
- Training offline RL agents using CQL, BCQ, or TD3+BC
- Benchmarking model compression tradeoffs (quantization, pruning, distillation)
- Real-time deployment of agents under latency and memory constraints

---

## ğŸ§  Environment Design

The environment is a 10Ã—10 gridworld with directional agent movement, obstacles, and a single terminal goal. Key features:

- ğŸ” **Action space**: Turn left, move forward, turn right (relative to current orientation)
- ğŸ‘ï¸ **Partial observability**: Agent receives a 3Ã—3 window centered on its current location
- ğŸ”¢ **Dual observation modes**:
  - **Image**: 3Ã—3 local grid (int matrix)
  - **Vector**: Agent position and goal coordinates as a flat vector
- ğŸ¯ **Reward structure**:
  - `+10` for reaching the goal (sparse)
  - `-0.1` per step (dense penalty)
- â›” **Obstacles**: Defined in the grid and block movement
- ğŸ–¥ï¸ **Real-time rendering**: PyGame visualization at 10 FPS

---

## ğŸ§  Replay Buffer Generation

We simulate scripted agents in the custom Gridworld environment to collect experience data for offline RL training.

Each transition includes:
- `observation`
- `action`
- `reward`
- `next_observation`
- `done`

These transitions are saved into a compressed `.npz` buffer (`dataset/replay_buffer.npz`) for later use.

To generate the dataset:

```bash
python dataset/collect.py --episodes 100
```

Outputs:
- âœ… `dataset/metadata.txt` â€“ Summary of average reward, length, and transitions
- ğŸ“Š `dataset/reward_histogram.png` â€“ Reward distribution histogram

Example stats:
```
Num Episodes: 100
Avg Episode Reward: 8.30
Avg Episode Length: 18.00
Total Transitions: 1800
```

---

## ğŸ“Š Dataset Visualizations

We visualize the replay buffer to verify coverage and distribution:

- ğŸŒ€ t-SNE of Observations: `docs/plots/tsne_obs.png`
- ğŸ® Action Distribution: `docs/plots/action_distribution.png`
- ğŸ¯ Reward Distribution: `docs/plots/episode_rewards.png`

Generate plots via:

```bash
python dataset/viz.py
```

---

## ğŸ‹ï¸ Training the CQL Agent

We implement a Conservative Q-Learning (CQL) agent using PyTorch. The agent is trained *offline* on the replay buffer.

Key Features:
- Vector observation space (4D: [agent_x, agent_y, goal_x, goal_y])
- Discrete action space with 3 actions
- Bellman loss, conservative loss, and optional behavior cloning (BC) loss

Training includes:
- âœ… Evaluation loop (avg Q, policy accuracy)
- âœ… TensorBoard + Matplotlib logging
- âœ… Best checkpoint saving (`checkpoints/`)

Run:

```bash
python agent/train.py
```

---

## ğŸ“ˆ Training Visualizations

### ğŸ“‰ Loss Curves (`docs/cql_training_losses.png`)

- **Bellman Loss**: TD error between predicted and target Q-values  
- **Conservative Loss**: Penalizes overestimation of unseen actions  
- **BC Loss**: Aligns policy with dataset behavior

![CQL Training Losses](docs/cql_training_losses.png)

### ğŸ§ª TensorBoard Logging

All metrics are tracked in TensorBoard:

```bash
tensorboard --logdir=logs
```

Access at: [http://localhost:6006](http://localhost:6006)

Track:
- [`Eval/PolicyAccuracy`](http://localhost:6006/#scalars&tagFilter=PolicyAccuracy)
- [`Eval/AvgQ`](http://localhost:6006/#scalars&tagFilter=AvgQ)
- [`Loss/BC`](http://localhost:6006/#scalars&tagFilter=Loss%2FBC)
- [`Loss/Bellman`](http://localhost:6006/#scalars&tagFilter=Loss%2FBellman)

---

## ğŸ® Agent Evaluation and Replay

After training, test the trained agent in the environment and save replay GIFs.

Run:

```bash
python agent/test_agent.py
```

Outputs:
- âœ… Printed reward over 10 episodes
- âœ… Replay saved as GIF: `docs/replays/test_run.gif`

Preview:

![Sample Replay](docs/replays/test_run.gif)

---

## ğŸ“ Project Structure

```bash
offline-rl-agent/
â”‚
â”œâ”€â”€ env/
â”‚   â””â”€â”€ neuroquant_env.py         # Custom gridworld environment
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ collect.py                # Data generation script
â”‚   â”œâ”€â”€ replay_buffer.npz         # Collected offline transitions
â”‚   â”œâ”€â”€ metadata.txt              # Episode stats
â”‚   â”œâ”€â”€ reward_histogram.png      # Reward histogram
â”‚   â”œâ”€â”€ viz.py                    # t-SNE, action, reward plots
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ cql_training_losses.png   # Training curves
â”‚   â””â”€â”€ replays/
â”‚       â””â”€â”€ test_run.gif          # GIF of trained agent behavior
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ cql.py                    # CQL agent logic
â”‚   â”œâ”€â”€ train.py                  # Training script
â”‚   â””â”€â”€ test_agent.py             # Inference and replay
â”‚
â”œâ”€â”€ checkpoints/                  # Saved model weights
â”‚   â”œâ”€â”€ best_q.pt
â”‚   â””â”€â”€ best_policy.pt
â”‚
â””â”€â”€ logs/                         # TensorBoard logs
```

---

## âœ… Summary

| Component | Description |
|----------|-------------|
| Env | Custom 10x10 gridworld, image/vector obs |
| Dataset | 100-episode buffer with 1800 transitions |
| Agent | CQL agent with offline training |
| Logging | TensorBoard + Matplotlib |
| Inference | Replay and metrics saved |
| Visuals | t-SNE, reward histogram, action dist |

---

## ğŸ§ª Try It Out

```bash
# Step-by-step
python dataset/collect.py --episodes 100
python dataset/viz.py
python agent/train.py
python agent/test_agent.py
```

---

## ğŸ“¬ Contact

Feel free to reach out or open an issue for any questions or ideas!

