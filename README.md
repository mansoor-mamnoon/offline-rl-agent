## ğŸš€ Project Overview

**NeuroQuant Agent** is a fully custom offline reinforcement learning benchmark, built from the ground up with real-time constraints, compression-aware inference, and deployment to latency-constrained environments.

The project begins with a custom-built 10Ã—10 gridworld environment that supports:

- ğŸ” **Directional movement**: The agent can turn left, go forward, or turn right relative to its current orientation.
- ğŸ‘ï¸ **Partial observability**: Instead of seeing the entire map, the agent receives a 3Ã—3 view centered around its position.
- â›” **Obstacles**: Impassable wall tiles block the agent's path and require navigation.
- ğŸ¯ **Goal tile**: A single terminal state gives a large positive reward when reached.
- ğŸ–¥ï¸ **Real-time PyGame rendering**: Each simulation step is rendered at 10 FPS for visual inspection and timing fidelity.

This environment supports:
- Offline dataset generation
- Training offline RL agents using CQL, BCQ, or TD3+BC
- Evaluating model compression tradeoffs (quantization, pruning, distillation)
- Deploying agents under strict latency and memory constraints

---

## ğŸ§  Environment Design

The environment is a 10Ã—10 gridworld with partial observability and discrete relative movement.

**Key Features:**
- ğŸ” Action space: `Turn Left`, `Move Forward`, `Turn Right`
- ğŸ‘ï¸ Observation: 3Ã—3 local view centered on the agent
- ğŸ”¢ Dual modes:
  - **Image** mode (local grid)
  - **Vector** mode (`[agent_x, agent_y, goal_x, goal_y]`)
- ğŸ¯ Reward structure:
  - `+10` for reaching the goal
  - `-0.1` per step
- â›” Obstacles block movement
- ğŸ–¥ï¸ Real-time PyGame rendering at 10 FPS

---

## ğŸ§  Replay Buffer Generation

We simulate random agents in the custom environment to collect experience for offline RL.

Each transition includes:
- `observation`
- `action`
- `reward`
- `next_observation`
- `done`

Generated data is saved in:
- âœ… `dataset/replay_buffer.npz` â€“ transition buffer
- ğŸ“ `dataset/metadata.txt` â€“ episode stats
- ğŸ“Š `dataset/reward_histogram.png` â€“ episode reward distribution

To generate the dataset, run:

```bash
python dataset/collect.py --episodes 100

```

## ğŸ“ Project Structure

```bash
offline-rl-agent/
â”‚
â”œâ”€â”€ env/                    # Custom Gym-like environment
â”‚   â””â”€â”€ neuroquant_env.py
â”‚
â”œâ”€â”€ dataset/                # Replay buffer + visualizations
â”‚   â”œâ”€â”€ collect.py
â”‚   â”œâ”€â”€ viz.py
â”‚   â”œâ”€â”€ replay_buffer.npz
â”‚   â”œâ”€â”€ reward_histogram.png
â”‚   â”œâ”€â”€ metadata.txt
â”‚
â”œâ”€â”€ agent/                  # CQL training code
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ cql.py
â”‚
â”œâ”€â”€ docs/                   # Visual assets
â”‚   â”œâ”€â”€ tsne_obs.png
â”‚   â”œâ”€â”€ action_distribution.png
â”‚   â”œâ”€â”€ episode_rewards.png
â”‚   â”œâ”€â”€ cql_training_losses.png
â”‚   â””â”€â”€ replays/
â”‚       â””â”€â”€ test_run.gif
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ run_env_test.py
```

---

## ğŸ“Š Dataset Visualizations

We visualize the replay buffer to verify coverage and diversity.

- ğŸŒ€ **t-SNE of State Vectors**  
  ![t-SNE](docs/plots/tsne_obs.png)

- ğŸ® **Action Distribution**  
  ![Action Histogram](docs/plots/action_distribution.png)

- ğŸ¯ **Episode Reward Distribution**  
  ![Rewards](docs/plots/episode_rewards.png)

To generate these plots, run:

```bash
python dataset/viz.py

```

---

## ğŸ‹ï¸ Training the CQL Agent

We train a Conservative Q-Learning (CQL) agent on the replay buffer using PyTorch.

**Agent Highlights:**
- Inputs: vector observations
- Discrete 3-action space
- 2-layer Q-network and policy
- Losses:
  - Bellman loss
  - Conservative penalty
  - (Optional) behavior cloning loss

Run training:

```bash
python agent/train.py
```

Logs print every 100 epochs with:
- Bellman loss
- Conservative loss
- BC loss (if enabled)

---

## ğŸ“‰ Training Loss Visualization

Below is the training loss of the CQL agent over 1000 epochs:

![CQL Training Losses](docs/cql_training_losses.png)

- **Bellman Loss**: TD error between predicted and target Q-values
- **Conservative Loss**: Penalizes high Q-values for unseen actions
- **Behavior Cloning Loss**: Aligns learned policy with dataset behavior

---

## ğŸ¥ Demos + GIFs

The environment supports rendering full episodes as GIFs via `render_episode_gif()`.

Example replay:

![Sample Replay](docs/replays/test_run.gif)

---

## ğŸ› ï¸ Getting Started

```bash
# Clone and enter repo
git clone https://github.com/mansoor-mamnoon/offline-rl-agent.git
cd offline-rl-agent

# Set up venv
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run environment manually
python run_env_test.py

# Collect data
python dataset/collect.py --episodes 100

# Visualize replay buffer
python dataset/viz.py

# Train the agent
python agent/train.py
```

---

Let me know if you'd like to add evaluation metrics, save checkpoints, or display learning curves in real-time!



